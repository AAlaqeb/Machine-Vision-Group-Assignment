# -*- coding: utf-8 -*-
"""Machine Vision Group Assignment Question 5 Depthwise.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jegqp8XchTc6nP3vXHDyx8xqfkUBmB_S
"""



# Importing the necessary libraries
import torch
from torch import nn

import torchvision
from torchvision import datasets, models
import torchvision.transforms as transforms
import numpy as np
import matplotlib.pyplot as plt
from google.colab import drive


# Mount Google Drive to access the dataset stored there
drive.mount('/content/drive')

# Define transformations for pre-processing images
transform = transforms.Compose(
    [ transforms.Resize((224,224)),
      transforms.CenterCrop(224),
      transforms.ToTensor(), # this to convert the images to 4D matrix (B,C,H,W)
      transforms.Normalize((0.485,0.456,0.406),(0.229,0.224,0.225))] # Normalize images with mean and std deviation
)

# Define paths to training and testing data
train_dir = '/content/drive/MyDrive/Machine Vision Group Assignment/Balls_10/train'
test_dir = '/content/drive/MyDrive/Machine Vision Group Assignment/Balls_10/test'

# Load datasets with defined transformations
train_data = datasets.ImageFolder(root = train_dir,transform = transform)
test_data = datasets.ImageFolder(root = test_dir,transform = transform)


class_names = train_data.classes # Get class names from the dataset
train_data.classes

from torch.utils.data import DataLoader
# Define data loaders with batch size
BATCH_SIZE = 4
train_dataloader = DataLoader(train_data,batch_size=BATCH_SIZE, shuffle=True)
test_dataloader = DataLoader(test_data,batch_size=BATCH_SIZE, shuffle=False)

# Print information about the data loaders
print(f"Dataloaders: {train_dataloader, test_dataloader}")
print(f"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}")
print(f"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}")

import numpy as np
import matplotlib.pyplot as plt

# Function to visualize the dataset
def show_image(img):
    img = img / 2 + 0.5     # unnormalize
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()

# Display some random training images
dataiter = iter(train_dataloader)
images, labels = next(dataiter)

show_image(torchvision.utils.make_grid(images)) # show images
print(' '.join('%5s' % class_names[labels[j]] for j in range(4))) # print labels

# Define the CNN model with depthwise convolution
class CNNModelDepthwise(nn.Module):
    def __init__(self):
        super(CNNModelDepthwise, self).__init__()
        # First convolutional layer with 3 input channels, 6 output channels, and a kernel size of 5
        self.conv1 = nn.Conv2d(3, 6, 5)
        # Batch normalization layer for the first convolutional layer
        self.batchnorm1 = nn.BatchNorm2d(6)
        # Max pooling layer with a kernel size of 2 and stride of 2 to reduce spatial dimensions
        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)

        # Depthwise separable convolution consists of a depthwise convolution followed by a pointwise convolution
        # Depthwise convolution: Applies a single filter per input channel (input and output channels are the same)
        self.depthwise = nn.Conv2d(6, 6, 5, groups=6)
        # Pointwise convolution: A standard convolution that changes the number of channels
        self.pointwise = nn.Conv2d(6, 16, 1)
         # Batch normalization layer for the depthwise and pointwise convolutions
        self.batchnorm2 = nn.BatchNorm2d(16)
         # Max pooling layer to reduce spatial dimensions after depthwise and pointwise convolutions
        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)
        
        # Third convolutional layer to further process the features
        self.conv3 = nn.Conv2d(16, 20, 3)
        self.batchnorm3 = nn.BatchNorm2d(20)
        self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)

        # Flatten layer to convert 2D feature maps into a 1D feature vector
        self.flatten = nn.Flatten()
        # Fully connected layer to map the flattened features to a 100-dimensional feature space
        self.fc1 = nn.Linear(20 * 25 * 25, 100)
        # Dropout layer with a dropout rate of 0.4 to prevent overfitting
        self.dropout = nn.Dropout(0.4)
         # Second fully connected layer that outputs 10-dimensional vectors, for the 10 classes
        self.fc2 = nn.Linear(100, 10)
         # ReLU activation function, used after convolutional and fully connected layers
        self.relu = nn.ReLU()
    
    
    # Define the forward pass through the network
    def forward(self, x):
        x = self.conv1(x)
        x = self.relu(x)
        x = self.batchnorm1(x)
        x = self.maxpool1(x)

        x = self.depthwise(x)
        x = self.pointwise(x)
        x = self.relu(x)
        x = self.batchnorm2(x)
        x = self.maxpool2(x)

        x = self.conv3(x)
        x = self.relu(x)
        x = self.batchnorm3(x)
        x = self.maxpool3(x)

        x = self.flatten(x)
        x = self.fc1(x)
        x = self.dropout(x)
        x = self.relu(x)
        out = self.fc2(x)

        return out

# Initialize the model and set it to training mode
model = CNNModelDepthwise()

model.train()

# Install and import torchinfo for model summary
!pip install torchinfo

from torchinfo import summary

summary(model)

# Define loss function and optimizer
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(),lr=0.001,momentum=0.9)#sgd:stochastic gradiant descent

# Move model to GPU 
model.to('cuda')

import time
from tqdm.auto import tqdm

# Define the training and validation function
def train_and_validate(model, loss_criterion, optimizer, train_dataloader, test_dataloader, epochs=25, device='cuda'):
    '''
    Function to train and validate
    Parameters
        :param model: Model to train and validate
        :param loss_criterion: Loss Criterion to minimize
        :param optimizer: Optimizer for computing gradients
        :param train_dataloader: DataLoader for training data
        :param test_dataloader: DataLoader for test/validation data
        :param epochs: Number of epochs (default=25)
        :param device: Device to perform computations ('cuda' or 'cpu')

    Returns
        model: Trained Model with best validation accuracy
        history: (dict object): Having training loss, accuracy and validation loss, accuracy
    '''

    start = time.time()
    history = []
    best_acc = 0.0

    for epoch in tqdm(range(epochs)):
        epoch_start = time.time()
        print("Epoch: {}/{}".format(epoch+1, epochs))

        model.train()

        train_loss = 0.0
        train_acc = 0.0

        valid_loss = 0.0
        valid_acc = 0.0

        for i, (inputs, labels) in enumerate(train_dataloader):

            inputs = inputs.to(device)
            labels = labels.to(device)

            # Clean existing gradients
            optimizer.zero_grad()

            # Forward pass - compute outputs on input data using the model
            outputs = model(inputs)

            # Compute loss
            loss = loss_criterion(outputs, labels)

            # Backpropagate the gradients
            loss.backward()

            # Update the parameters
            optimizer.step()

            # Compute the total loss for the batch and add it to train_loss
            train_loss += loss.item() * inputs.size(0)

            # Compute the accuracy
            ret, predictions = torch.max(outputs.data, 1)
            correct_counts = predictions.eq(labels.data.view_as(predictions))

            # Convert correct_counts to float and then compute the mean
            acc = torch.mean(correct_counts.type(torch.FloatTensor))

            # Compute total accuracy in the whole batch and add to train_acc
            train_acc += acc.item() * inputs.size(0)

        # Validation - No gradient tracking needed
        with torch.no_grad():

            model.eval()

            # Validation loop
            for j, (inputs, labels) in enumerate(test_dataloader):
                inputs = inputs.to(device)
                labels = labels.to(device)

                # Forward pass - compute outputs on input data using the model
                outputs = model(inputs)

                # Compute loss
                loss = loss_criterion(outputs, labels)

                # Compute the total loss for the batch and add it to valid_loss
                valid_loss += loss.item() * inputs.size(0)

                # Calculate validation accuracy
                ret, predictions = torch.max(outputs.data, 1)
                correct_counts = predictions.eq(labels.data.view_as(predictions))

                # Convert correct_counts to float and then compute the mean
                acc = torch.mean(correct_counts.type(torch.FloatTensor))

                # Compute total accuracy in the whole batch and add to valid_acc
                valid_acc += acc.item() * inputs.size(0)


        # Find average training loss and training accuracy
        avg_train_loss = train_loss / len(train_dataloader.dataset)
        avg_train_acc = train_acc / len(train_dataloader.dataset)

        # Find average validation loss and training accuracy
        avg_test_loss = valid_loss / len(test_dataloader.dataset)
        avg_test_acc = valid_acc / len(test_dataloader.dataset)

        history.append([avg_train_loss, avg_test_loss, avg_train_acc, avg_test_acc])

        epoch_end = time.time()

        print("Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \n\t\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s".format(epoch, avg_train_loss, avg_train_acc * 100, avg_test_loss, avg_test_acc * 100, epoch_end - epoch_start))

        # Save if the model has best accuracy till now
        if avg_test_acc > best_acc:
            best_acc = avg_test_acc
            best_model = model
            torch.save(best_model, 'best_model.pt')

    return best_model, history

# Check CUDA availability
import torch
if torch.cuda.is_available():
    print("CUDA is available. You can use GPU.")
else:
    print("CUDA is not available. You can only use CPU.")

# Train the model
num_epochs = 10
trained_model, history = train_and_validate(model,loss_fn,optimizer,train_dataloader,test_dataloader,num_epochs)

# Plot the loss curve

def plot_loss(history):
  history = np.array(history)
  plt.plot(history[:,0:2])
  plt.legend(['Tr Loss', 'Val Loss'])
  plt.xlabel('Epoch Number')
  plt.ylabel('Loss')
  plt.ylim(0,3)
  plt.show()

plot_loss(history)

# Plot the accuracy curve

def plot_accuracy(history):
  history = np.array(history)
  plt.plot(history[:,2:4])
  plt.legend(['Tr Accuracy', 'Val Accuracy'])
  plt.xlabel('Epoch Number')
  plt.ylabel('Accuracy')
  plt.ylim(0,1)
  plt.show()

plot_accuracy(history)

from sklearn.metrics import confusion_matrix
import seaborn as sn
import pandas as pd

# Function to plot confusion matrix
def plot_confusionMatrix(model, test_dataloader):

  y_pred = []
  y_true = []

  model.to('cpu')

  # iterate over test data
  for inputs, labels in test_dataloader:
          output = model(inputs) # Feed Network

          output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()
          y_pred.extend(output) # Save Prediction

          labels = labels.data.cpu().numpy()
          y_true.extend(labels) # Save Truth

  # Build confusion matrix
  cf_matrix = confusion_matrix(y_true, y_pred)
  df_cm = pd.DataFrame(cf_matrix/np.sum(cf_matrix) *10, index = [i for i in class_names],
                      columns = [i for i in class_names])
  plt.figure(figsize = (20,10))
  sn.heatmap(df_cm, annot=True)

plot_confusionMatrix(model, test_dataloader)
