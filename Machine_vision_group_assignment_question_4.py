# -*- coding: utf-8 -*-
"""Machine Vision Group Assignment Question 4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J4W5xiXusf7KZ2oTbuVPGVIJCeVuKrkM
"""

# Importing the necessary libraries
import torch
from torch import nn
import torchvision
from torchvision import datasets, models
import torchvision.transforms as transforms
import numpy as np
import matplotlib.pyplot as plt
from torch.optim.lr_scheduler import StepLR, ExponentialLR
from torch.utils.data import DataLoader
import time
from tqdm.auto import tqdm
import pandas as pd
from sklearn.metrics import confusion_matrix
import seaborn as sn
from torch.optim.lr_scheduler import CosineAnnealingLR
from google.colab import drive

# Mounting Google Drive for accessing datasets
drive.mount('/content/drive')

# Define image transformations for pre-processing
transform = transforms.Compose(
    [ transforms.Resize((224,224)),
      transforms.CenterCrop(224),
      transforms.ToTensor(), # this to convert the images to 4D matrix (B,C,H,W)
      transforms.Normalize((0.485,0.456,0.406),(0.229,0.224,0.225))] # Normalize images with mean and std deviation
)

# Set directories for training and testing datasets
train_dir = '/content/drive/MyDrive/Machine Vision Group Assignment/Balls_10/train'
test_dir = '/content/drive/MyDrive/Machine Vision Group Assignment/Balls_10/test'

# Load datasets
train_data = datasets.ImageFolder(root = train_dir,transform = transform)
test_data = datasets.ImageFolder(root = test_dir,transform = transform)

# Define data loaders with batch size
BATCH_SIZE = 4
train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)
test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)

# Print information about the data loaders
print(f"Dataloaders: {train_dataloader, test_dataloader}")
print(f"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}")
print(f"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}")

class_names = train_data.classes # Get class names from the dataset
train_data.classes

# Function to display an image
def show_image(img):
    img = img / 2 + 0.5     # unnormalize
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()

# Display some random training images
dataiter = iter(train_dataloader)
images, labels = next(dataiter)

show_image(torchvision.utils.make_grid(images)) # show images
print(' '.join('%5s' % class_names[labels[j]] for j in range(4))) # print labels

# Define a Convolutional Neural Network model
class CNNModel(nn.Module):
    
    def __init__(self):
      super(CNNModel,self).__init__()
      
      # Define layers and operations
      self.conv1= nn.Conv2d(3,6,5)
      self.conv2 = nn.Conv2d(6,16,5)
      self.conv3 = nn.Conv2d(16,20,3)

      self.maxpool1 = nn.MaxPool2d(kernel_size=2,stride=2)
      self.maxpool2 = nn.MaxPool2d(kernel_size=2,stride=2)
      self.maxpool3 = nn.MaxPool2d(kernel_size=2,stride=2)

      self.fc1 = nn.Linear(20*25*25,100)
      self.fc2 = nn.Linear(100,10)
      self.flatten= nn.Flatten()

      self.batchnorm1 = nn.BatchNorm2d(6)
      self.batchnorm2 = nn.BatchNorm2d(16)
      self.batchnorm3 = nn.BatchNorm2d(20)
      self.dropout = nn.Dropout(0.4)
      self.relu = nn.ReLU() 

    # Define forward pass
    def forward(self,x):
      x = self.conv1(x)
      x = self.relu(x)
      x = self.batchnorm1(x) 
      x = self.maxpool1(x)
      x = self.conv2(x)
      x = self.relu(x)
      x = self.batchnorm2(x)
      x = self.maxpool2(x)
      x = self.conv3(x)
      x = self.relu(x)
      x = self.batchnorm3(x)
      x = self.maxpool3(x)
      x = self.flatten(x)
      x = self.fc1(x)
      x = self.dropout(x)
      x = self.relu(x)
      out = self.fc2(x)

      return out

# Initialize the model and set to training mode
model = CNNModel()

model.train()

# Install and import torchinfo for model summary
!pip install torchinfo

from torchinfo import summary

summary(model)

# Define loss function and optimizer
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to GPU if available
model.to('cuda')


# Define learning rate schedulers
scheduler_step = StepLR(optimizer, step_size=5, gamma=0.1)
scheduler_exp = ExponentialLR(optimizer, gamma=0.95)

# Function to train and validate the model
def train_and_validate(model, loss_criterion, optimizer, scheduler, train_dataloader, test_dataloader, epochs=25, device='cuda'):
    '''
    Function to train and validate
    Parameters
        :param model: Model to train and validate
        :param loss_criterion: Loss Criterion to minimize
        :param optimizer: Optimizer for computing gradients
        :param train_dataloader: DataLoader for training data
        :param test_dataloader: DataLoader for test/validation data
        :param epochs: Number of epochs (default=25)
        :param device: Device to perform computations ('cuda' or 'cpu')

    Returns
        model: Trained Model with best validation accuracy
        history: (dict object): Having training loss, accuracy and validation loss, accuracy
    '''

    start = time.time()
    history = []
    best_acc = 0.0

    for epoch in tqdm(range(epochs)):
        epoch_start = time.time()
        print("Epoch: {}/{}".format(epoch+1, epochs))

        model.train()

        train_loss = 0.0
        train_acc = 0.0

        valid_loss = 0.0
        valid_acc = 0.0

        for i, (inputs, labels) in enumerate(train_dataloader):

            inputs = inputs.to(device)
            labels = labels.to(device)

            # Clean existing gradients
            optimizer.zero_grad()

            # Forward pass - compute outputs on input data using the model
            outputs = model(inputs)

            # Compute loss
            loss = loss_criterion(outputs, labels)

            # Backpropagate the gradients
            loss.backward()

            # Update the parameters
            optimizer.step()

            # Compute the total loss for the batch and add it to train_loss
            train_loss += loss.item() * inputs.size(0)

            # Compute the accuracy
            ret, predictions = torch.max(outputs.data, 1)
            correct_counts = predictions.eq(labels.data.view_as(predictions))

            # Convert correct_counts to float and then compute the mean
            acc = torch.mean(correct_counts.type(torch.FloatTensor))

            # Compute total accuracy in the whole batch and add to train_acc
            train_acc += acc.item() * inputs.size(0)

        # Validation - No gradient tracking needed
        with torch.no_grad():

            model.eval()

            # Validation loop
            for j, (inputs, labels) in enumerate(test_dataloader):
                inputs = inputs.to(device)
                labels = labels.to(device)

                # Forward pass - compute outputs on input data using the model
                outputs = model(inputs)

                # Compute loss
                loss = loss_criterion(outputs, labels)

                # Compute the total loss for the batch and add it to valid_loss
                valid_loss += loss.item() * inputs.size(0)

                # Calculate validation accuracy
                ret, predictions = torch.max(outputs.data, 1)
                correct_counts = predictions.eq(labels.data.view_as(predictions))

                # Convert correct_counts to float and then compute the mean
                acc = torch.mean(correct_counts.type(torch.FloatTensor))

                # Compute total accuracy in the whole batch and add to valid_acc
                valid_acc += acc.item() * inputs.size(0)


        # Find average training loss and training accuracy
        avg_train_loss = train_loss / len(train_dataloader.dataset)
        avg_train_acc = train_acc / len(train_dataloader.dataset)

        # Find average validation loss and training accuracy
        avg_test_loss = valid_loss / len(test_dataloader.dataset)
        avg_test_acc = valid_acc / len(test_dataloader.dataset)

        history.append([avg_train_loss, avg_test_loss, avg_train_acc, avg_test_acc])

        epoch_end = time.time()

        print("Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \n\t\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s".format(epoch, avg_train_loss, avg_train_acc * 100, avg_test_loss, avg_test_acc * 100, epoch_end - epoch_start))

        # Save if the model has best accuracy till now
        if avg_test_acc > best_acc:
            best_acc = avg_test_acc
            best_model = model
            torch.save(best_model, 'best_model.pt')

    return best_model, history

# Check if CUDA is available
import torch
if torch.cuda.is_available():
    print("CUDA is available. You can use GPU.")
else:
    print("CUDA is not available. You can only use CPU.")

# Train the model with Step Decay learning rate
epochs = 10
trained_model_step, history_step = train_and_validate(model, loss_fn, optimizer, scheduler_step, train_dataloader, test_dataloader, epochs)

# Reset model and optimizer for Exponential Decay training
model = CNNModel().to('cuda')
optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Train the model with Exponential Decay learning rate
trained_model_exp, history_exp = train_and_validate(model, loss_fn, optimizer, scheduler_exp, train_dataloader, test_dataloader, epochs=10)

# Convert histories to numpy arrays
history_step = np.array(history_step)
history_exp = np.array(history_exp)

# Create a comparative table
comparison_table = pd.DataFrame({
    "Epoch": range(1, 11),
    "Step Decay Accuracy": history_step[:, 3],
    "Exponential Decay Accuracy": history_exp[:, 3]
})

print(comparison_table)

# Plot the loss curve

def plot_loss(history_step):
  history_step = np.array(history_step)
  plt.plot(history_step[:,0:2])
  plt.legend(['Tr Loss', 'Val Loss'])
  plt.xlabel('Epoch Number')
  plt.ylabel('Loss')
  plt.ylim(0,3)
  plt.show()

plot_loss(history_step)

# Plot the accuracy curve

def plot_accuracy(history_step):
  history_step = np.array(history_step)
  plt.plot(history_step[:,2:4])
  plt.legend(['Tr Accuracy', 'Val Accuracy'])
  plt.xlabel('Epoch Number')
  plt.ylabel('Accuracy')
  plt.ylim(0,1)
  plt.show()

plot_accuracy(history_step)


# Plot the loss curve (Exponential)

def plot_loss(history_exp):
  history_exp = np.array(history_exp)
  plt.plot(history_exp[:,0:2])
  plt.legend(['Tr Loss', 'Val Loss'])
  plt.xlabel('Epoch Number')
  plt.ylabel('Loss')
  plt.ylim(0,3)
  plt.show()

plot_loss(history_exp)

# Plot the accuracy curve (Exponential)

def plot_accuracy(history_exp):
  history_exp = np.array(history_exp)
  plt.plot(history_exp[:,2:4])
  plt.legend(['Tr Accuracy', 'Val Accuracy'])
  plt.xlabel('Epoch Number')
  plt.ylabel('Accuracy')
  plt.ylim(0,1)
  plt.show()

plot_accuracy(history_exp)
